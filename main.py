import asyncio
import sys
import pandas as pd
import glob
import os
import re
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup 
from src.scraper.naver_shopping_scraper import NaverShoppingScraper
from src.analyzer.keyword_analyzer import KeywordAnalyzer
import config

from playwright.async_api import async_playwright

class ProductTitleFetcher:
    """
    URLì—ì„œ ìƒí’ˆëª…ì„ ì¶”ì¶œí•˜ê³  ì •ì œí•˜ëŠ” í´ë˜ìŠ¤ (Mobile Playwright + iPhone 13 Pro)
    """
    @staticmethod
    async def fetch_and_clean(url: str) -> str:
        # 1. PC URL -> Mobile URL ë³€í™˜ (ì†ë„ ë° êµ¬ì¡° ë‹¨ìˆœí™”)
        if "smartstore.naver.com" in url and "m.smartstore.naver.com" not in url:
            url = url.replace("smartstore.naver.com", "m.smartstore.naver.com")
            
        print(f"URLì—ì„œ ìƒí’ˆëª… ì¶”ì¶œ ì¤‘ (Mobile Playwright)... {url}")
        title = ""
        
        try:
            async with async_playwright() as p:
                # 2. Device Emulation (Step 1 ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ í™˜ê²½ êµ¬ì„±)
                iphone_13 = p.devices['iPhone 13 Pro']
                
                browser = await p.chromium.launch(
                    headless=False, # ë³´ì•ˆ ìš°íšŒë¥¼ ìœ„í•´ Headless=False ìœ ì§€
                    args=["--disable-blink-features=AutomationControlled"]
                )
                
                context = await browser.new_context(
                    **iphone_13,
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )
                
                # Stealth: navigator.webdriver ìˆ¨ê¸°ê¸°
                await context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                """)
                
                page = await context.new_page()

                # 3. Resource Optimization (ì´ë¯¸ì§€, í°íŠ¸ ì°¨ë‹¨ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
                await page.route("**/*", lambda route: route.abort() 
                    if route.request.resource_type in ["image", "media", "font"] 
                    else route.continue_()
                )

                try:
                    await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                    
                    # 4. Title Extraction
                    # ìš°ì„ ìˆœìœ„ 1: Open Graph Meta Tag (ê°€ì¥ ê¹”ë”í•¨)
                    og_title_loc = page.locator('meta[property="og:title"]')
                    if await og_title_loc.count() > 0:
                        title = await og_title_loc.first.get_attribute("content")
                    
                    # ìš°ì„ ìˆœìœ„ 2: Page Title (Fallback)
                    if not title:
                        title = await page.title()

                except Exception as e:
                    print(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ (ì œëª©ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤): {e}")
                finally:
                    await browser.close()
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return ""

        if not title:
            return ""

        print(f"ì›ì²œ ìƒí’ˆëª…: {title}")
        return ProductTitleFetcher.clean_title(title)

    @staticmethod
    def clean_title(title: str) -> str:
        # 0. ì ‘ë¯¸ì‚¬ ì •ë¦¬ (íƒ€ì´í‹€ íƒœê·¸ ë“±ì—ì„œ ë¶™ëŠ” ì¡ë‹¤í•œ ë¬¸êµ¬ ì œê±°)
        # ì˜ˆ: "ìƒí’ˆëª… : ë„¤ì´ë²„ ì‡¼í•‘", "ìƒí’ˆëª… : ì¥´ë¦¬ì”¨"
        title = re.sub(r' : ë„¤ì´ë²„.*', '', title)
        title = re.sub(r' : \S+', '', title) # " : ì‡¼í•‘ëª°ëª…" íŒ¨í„´ ì œê±° ì‹œë„

        # 1. ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±° (ëŒ€ê´„í˜¸ [], ì†Œê´„í˜¸ ())
        title = re.sub(r'\[.*?\]', '', title)
        title = re.sub(r'\(.*?\)', '', title)
        
        # 2. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë“±ì—ì„œ ë¶™ëŠ” ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì¶”ê°€ ì²˜ë¦¬
        title = title.replace("ë„¤ì´ë²„ì‡¼í•‘", "")

        # 3. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        title = " ".join(title.split())
        
        print(f"ì •ì œëœ í‚¤ì›Œë“œ: {title}")
        return title

async def main():
    print("=== J-Ops SEO Sniper ===")
    print("1. ê²€ìƒ‰ í‚¤ì›Œë“œ ì§ì ‘ ì…ë ¥")
    print("2. ë‚´ ìƒí’ˆ URL ì…ë ¥ (ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ)")
    
    mode = input("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (1/2): ").strip()
    keyword = ""

    if mode == "1":
        keyword = input("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    elif mode == "2":
        url = input("ìƒí’ˆ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not url.startswith("http"):
            print("ì˜¬ë°”ë¥¸ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        keyword = await ProductTitleFetcher.fetch_and_clean(url)
    else:
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
        return

    if not keyword:
        print("í‚¤ì›Œë“œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    print(f"\n>>> '{keyword}' í‚¤ì›Œë“œë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # -------------------------------------------------------------
    # Step 1: Scraper Execution
    # -------------------------------------------------------------
    scraper = NaverShoppingScraper(headless=False) # Headless False to avoid blocking
    products = await scraper.search(keyword)
    
    result_filename = ""
    
    if products:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_filename = config.RAW_DATA_DIR / f"results_{timestamp}.csv"
        
        # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±
        data = []
        for idx, product in enumerate(products, 1):
            data.append({
                'ìˆœìœ„': idx,
                'ìƒí’ˆëª…': product.title,
                'ê°€ê²©': product.price,
                'ì‡¼í•‘ëª°ëª…': product.store_name,
                'URL': product.url,
                'is_ad': product.is_ad,
                'íŒë§¤ì_ì„¤ì •_íƒœê·¸': product.tags
            })
        
        df = pd.DataFrame(data)
        columns = ['ìˆœìœ„', 'ìƒí’ˆëª…', 'ê°€ê²©', 'ì‡¼í•‘ëª°ëª…', 'íŒë§¤ì_ì„¤ì •_íƒœê·¸', 'URL', 'is_ad']
        df = df[columns]
        df.to_csv(result_filename, index=False, encoding="utf-8-sig")
        print(f"\n[Step 1 ì™„ë£Œ] ìˆ˜ì§‘ëœ ë°ì´í„°: {len(products)}ê±´ -> {result_filename}")
    else:
        print("ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # -------------------------------------------------------------
    # Step 2: Keyword Analysis
    # -------------------------------------------------------------
    # -------------------------------------------------------------
    # Step 2: Keyword Analysis
    # -------------------------------------------------------------
    print("\n[Step 2 ì‹œì‘] í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    
    analyzer = KeywordAnalyzer()
    
    # Generate timestamped report filename to avoid Permission denied errors
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S") # Re-generate or reuse? 
    # Better to match the result filename if possible, but simpler to just generate new one or extract from filename
    # Let's extract from result_filename to keep them paired: results_2025... -> keyword_report_2025...
    
    
    report_filename = "keyword_report.csv" # default fallback
    
    # We work with Path objects now
    result_path_obj = result_filename if isinstance(result_filename, Path) else Path(result_filename)
    base_name = result_path_obj.name
    
    if base_name.startswith("results_"):
        report_base = base_name.replace("results_", "keyword_report_")
    else:
        report_base = f"keyword_report_{timestamp}.csv"
        
    report_filename = config.REPORTS_DIR / report_base

    report_path = analyzer.analyze_file(str(result_filename), output_path=str(report_filename))
    
    if report_path:
        print(f"[Step 2 ì™„ë£Œ] ë¶„ì„ ë¦¬í¬íŠ¸: {report_path}")
        
        # -------------------------------------------------------------
        # Step 3: Tag Analysis
        # -------------------------------------------------------------
        print("\n[Step 3 ì‹œì‘] íƒœê·¸ ë¶„ì„ ì¤‘...")
        tag_report_base = "tag_report.csv"
        if base_name.startswith("results_"):
             tag_report_base = base_name.replace("results_", "tag_report_")
        else:
             tag_report_base = f"tag_report_{timestamp}.csv"
             
        tag_report_filename = config.REPORTS_DIR / tag_report_base
        
        tag_report_path = analyzer.analyze_tags(str(result_filename), output_path=str(tag_report_filename))
        if tag_report_path:
             print(f"[Step 3 ì™„ë£Œ] íƒœê·¸ ë¦¬í¬íŠ¸: {tag_report_path}")
        
        # Final Output: Top 10 Keywords + Mention Tags
        try:
            report_df = pd.read_csv(report_path)
            print("\n" + "="*40)
            print(f"ğŸ“¢ '{keyword}' ê´€ë ¨ ì¶”ì²œ í™©ê¸ˆ í‚¤ì›Œë“œ TOP 10")
            print("="*40)
            
            top_10 = report_df.head(10)
            for idx, row in top_10.iterrows():
                print(f"{row['ìˆœìœ„']}. {row['í‚¤ì›Œë“œ']} (ë“±ì¥: {row['ë“±ì¥íšŸìˆ˜']}íšŒ, ê´€ë ¨ìƒí’ˆ: {row['ê´€ë ¨_ìƒí’ˆìˆ˜']}ê°œ)")
            print("="*40)
            print(f"â€» ìƒì„¸ ë°ì´í„°: {result_filename}")
            print(f"â€» í‚¤ì›Œë“œ ë³´ê³ ì„œ: {report_path}")
            print(f"â€» íƒœê·¸ ë³´ê³ ì„œ: {tag_report_path}")
            
        except Exception as e:
            print(f"ê²°ê³¼ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print("í‚¤ì›Œë“œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        # if sys.platform == 'win32':
        #      asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

